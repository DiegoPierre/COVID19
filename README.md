# üêä Analyse des esp√®ces mondiales de crocodiles

## Aper√ßu du projet

Ce projet utilise un ensemble de donn√©es sur les crocodiles du monde entier afin d'analyser leurs caract√©ristiques biologiques et g√©ographiques.  
L'objectif est de comprendre la distribution des esp√®ces, de pr√©dire certaines caract√©ristiques comme la taille adulte ou le poids, et d'identifier les facteurs les plus importants pour la conservation et la gestion des populations.  

Le dataset contient des informations sur :  
- Le type et la classification de l'esp√®ce  
- Les caract√©ristiques biologiques (taille, poids, √¢ge √† maturit√©)  
- La r√©partition g√©ographique  
- Les observations enregistr√©es dans le monde entier

Ce projet permettra de construire des mod√®les de r√©gression et de classification, d'√©valuer leur performance et de fournir des recommandations pratiques pour la conservation des crocodiles.


###  Compr√©hension du m√©tier (Business Understanding)

**Probl√©matique principale :**  
Comment pr√©dire certaines caract√©ristiques des crocodiles (taille, poids, classification, etc.) et identifier les facteurs influen√ßant leur distribution dans le monde, afin de soutenir la recherche et la conservation des esp√®ces‚ÄØ?

**Parties prenantes :**  
- Chercheurs et biologistes sp√©cialis√©s en crocodiles  
- Organisations de protection de la faune  
- Data scientists et analystes de donn√©es  

**Objectifs du projet :**  
- Construire des mod√®les supervis√©s pour pr√©dire la classification ou des caract√©ristiques biologiques des crocodiles (r√©gression ou classification).  
- Identifier les facteurs influen√ßant la taille, le poids et la r√©partition des esp√®ces.  
- Fournir des recommandations exploitables pour la recherche et la conservation.

### Data Understanding

L'objectif de cette √©tape est de comprendre la structure et le contenu du dataset avant de commencer la pr√©paration et la mod√©lisation.

L'ensemble de donn√©es `crocodile_dataset.csv` contient des informations sur plusieurs esp√®ces de crocodiles dans le monde. Chaque enregistrement inclut :  
- Le type et la classification de l'esp√®ce (genre, esp√®ce, famille)  
- Les caract√©ristiques biologiques (taille adulte, poids, √¢ge √† maturit√©, etc.)  
- La r√©partition g√©ographique et habitats naturels  
- Les observations et mesures enregistr√©es par les chercheurs  
- Les comportements et interactions avec l‚Äôenvironnement  
- Les √©ventuelles notes ou commentaires des chercheurs  

**Objectifs de l'exploration des donn√©es :**  
- Identifier les colonnes **num√©riques** et **cat√©gorielles**  
- D√©tecter les **valeurs manquantes ou aberrantes**  
- Comprendre la **distribution des esp√®ces, tailles et poids**  
- Fournir un **premier aper√ßu des relations entre les variables**, qui guidera la pr√©paration des donn√©es et le choix des mod√®les  

Cette √©tape permettra de pr√©parer un **jeu de donn√©es propre et exploitable** pour la mod√©lisation supervis√©e (r√©gression ou classification) et pour des analyses exploratoires comme le clustering ou la visualisation g√©ographique.


###  Data Preparation

Cette √©tape vise √† pr√©parer le dataset pour la mod√©lisation supervis√©e et non supervis√©e.  

**√âtapes r√©alis√©es :**  
1. **Nettoyage des donn√©es**  
   - Suppression des doublons pour √©viter les biais dans les mod√®les.  

2. **Gestion des valeurs manquantes**  
   - Colonnes num√©riques : remplissage avec la m√©diane.  
   - Colonnes cat√©gorielles : remplissage avec la valeur la plus fr√©quente.  

3. **Transformation des variables**  
   - Conversion des colonnes de type texte ou date en formats exploitables (ex. ann√©e de naissance, √¢ge, etc.)  
   - Cr√©ation de nouvelles features si n√©cessaire (ex. ratio poids/taille, √¢ge relatif).  

4. **Encodage des variables cat√©gorielles**  
   - Transformation des colonnes telles que `Genre`, `Famille`, `Habitat` en valeurs num√©riques √† l‚Äôaide de `LabelEncoder` ou `OneHotEncoder`.  

5. **Mise √† l‚Äô√©chelle des features num√©riques**  
   - Standardisation des colonnes comme `Taille adulte`, `Poids`, `√Çge √† maturit√©` pour les mod√®les sensibles √† l‚Äô√©chelle (KNN, r√©seaux neuronaux).  

6. **S√©paration des donn√©es**  
   - Cr√©ation des jeux `X_train`, `X_test`, `y_train`, `y_test` selon la variable cible choisie (ex. classification de l‚Äôesp√®ce ou pr√©diction du poids).  

Cette pr√©paration garantit que les mod√®les de classification, r√©gression, clustering et analyses avanc√©es puissent √™tre appliqu√©s efficacement et produire des r√©sultats fiables et interpr√©tables.

# üìå R√©sum√© du code d'importation des biblioth√®ques

Le code commence par importer toutes les biblioth√®ques n√©cessaires pour :

---

## üîπ 1. Manipuler les donn√©es
- **pandas** : pour charger, transformer et analyser les donn√©es tabulaires.  
- **numpy** : pour les calculs num√©riques et la manipulation de tableaux.

---

## üîπ 2. Visualiser les r√©sultats
- **matplotlib** : pour cr√©er des graphiques simples (courbes, barres, histogrammes).  
- **seaborn** : pour des visualisations plus esth√©tiques et avanc√©es (heatmaps, boxplots, corr√©lations).

---

## üîπ 3. Pr√©parer et entra√Æner des mod√®les de Machine Learning
- **train_test_split** : s√©parer le dataset en un ensemble d‚Äôentra√Ænement et un ensemble de test.  
- **LabelEncoder** : transformer les variables cat√©gorielles en valeurs num√©riques.  
- **StandardScaler** : normaliser les donn√©es pour rendre les variables comparables.  

- **RandomForestClassifier / Regressor** : mod√®les d‚Äôarbres de d√©cision robustes pour classification et r√©gression.  
- **KNeighborsClassifier / Regressor** : algorithme bas√© sur la proximit√© des points (k plus proches voisins).  

- **GridSearchCV / RandomizedSearchCV** : optimisation des hyperparam√®tres des mod√®les.

---

## üîπ 4. √âvaluer les performances des mod√®les
- **Classification** :  
  - accuracy_score  
  - precision_score  
  - recall_score  
  - f1_score  
  - confusion_matrix  
  - classification_report  

- **R√©gression** :  
  - mean_squared_error (MSE)  
  - r2_score (coefficient de d√©termination)

``` python
# =========================
# Import des librairies
# =========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, mean_squared_error, r2_score
```


# üìå Lecture du dataset

Le code permet de charger et d‚Äôavoir un premier aper√ßu des donn√©es.

---

## üîπ 1. Chargement du dataset
```python
# =========================
data = pd.read_csv("crocodile_dataset.csv")

# Affichage des premi√®res lignes
data.head(10)

```

# üßπ Nettoyage des doublons

Le but est de v√©rifier si certaines lignes du dataset sont pr√©sentes plusieurs fois et, si c‚Äôest le cas, de les supprimer afin d‚Äô√©viter les biais dans l‚Äôanalyse.

---

## üîπ 1. V√©rification du nombre de doublons
```python
# Nettoyage des doublons
# =========================
print("Nombre de doublons :", data.duplicated().sum())

# Suppression des doublons
data = data.drop_duplicates()

# V√©rification
print("Nouvelle dimension du dataset :", data.shape)

```

# üß© Gestion des valeurs manquantes

L‚Äôobjectif est de traiter les valeurs manquantes (`NaN`) dans le dataset afin de garantir la qualit√© des analyses et d‚Äô√©viter les erreurs lors de l‚Äôentra√Ænement des mod√®les.

---

## üîπ 1. Identification des colonnes num√©riques
```python
# =========================
#Gestion des valeurs manquantes
# =========================

# Colonnes num√©riques
numeric_cols = data.select_dtypes(include=['float64','int64']).columns
for col in numeric_cols:
    data[col] = data[col].fillna(data[col].median())

# Colonnes cat√©gorielles
categorical_cols = data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    data[col] = data[col].fillna(data[col].mode()[0])

# V√©rification
print("Valeurs manquantes par colonne apr√®s traitement :")
print(data.isnull().sum())

```

